# OpenAI vs Gemini Embedding モデル比較調査

本ドキュメントでは、RAGシステムにおけるEmbedding（ベクトル化）モデルの選定基準として、OpenAIの `text-embedding-3-small` と Google Geminiの最新モデル `gemini-embedding-001` の比較調査結果をまとめる。

## 目次

- [1. 現状の構成と目的](#1-現状の構成と目的)
- [2. モデル比較表](#2-モデル比較表)
- [3. 詳細解説](#3-詳細解説)
  - [3.1 ベクトル化できる文章量（入力トークン制限）](#31-ベクトル化できる文章量入力トークン制限)
  - [3.2 バッチ処理（スピードとスループット）](#32-バッチ処理スピードとスループット)
  - [3.3 次元数とデータベース容量・精度](#33-次元数とデータベース容量精度)
  - [3.4 コストパフォーマンス](#34-コストパフォーマンス)
- [4. 結論と推奨](#4-結論と推奨)

---

## 1. 現状の構成と目的

現在、本プロジェクト（`gemini3_rag_qa`）では、以下の構成でEmbedding処理を実装している。

-   **使用モデル**: `gemini-embedding-001` (Google)
-   **設定**: デフォルトの **3072次元** を採用
-   **実装**: `helper_embedding.py` にてOpenAI互換のインターフェースでラップし、Qdrantへの格納を行っている。

本調査は、この構成の妥当性を再確認し、OpenAIモデルとの差異を明確にすることを目的とする。

---

## 2. モデル比較表

最新のモデル仕様（2025年時点）に基づく比較結果は以下の通りである。

| 比較項目 | OpenAI<br>(text-embedding-3-small) | Gemini<br>(gemini-embedding-001) | 備考・勝者 |
| :--- | :--- | :--- | :--- |
| **最大入力トークン数** | **8,191 tokens** | 2,048 tokens | **OpenAI**<br>OpenAIは約4倍長いテキストを一度に処理可能。 |
| **一度のバッチ上限数** | **2,048 件** (配列) | 100 件 (batchEmbedContents) | **OpenAI**<br>Geminiは小分けにしてリクエストする必要がある。 |
| **出力次元数** | 1536 (短縮可能) | **デフォルト 3072**<br>(MRLにより 768, 1536 へ短縮可能) | **Gemini**<br>より高次元で詳細な表現が可能。MRLにより柔軟性も高い。 |
| **コスト (Paid)** | **$0.02 / 1M tokens** | ~$0.025 / 1M **chars**<br>(Vertex AI) | **OpenAI**<br>1トークン≒4文字換算でOpenAIが安価。<br>※AI Studioなら無料枠あり。 |
| **日本語性能 (MTEB)** | 高い | **非常に高い** | **Gemini**<br>Googleの最新モデルとして多言語検索タスクで非常に優秀。 |
| **API速度 (レイテンシ)** | 非常に高速・安定 | 高速 | **OpenAI**<br>バッチ処理効率も含めるとOpenAIがスループットを出しやすい。 |

---

## 3. 詳細解説

### 3.1 ベクトル化できる文章量（入力トークン制限）

-   **OpenAI (8191 tokens)**
    -   非常に長いドキュメントや、複数のチャンクを結合したテキストも余裕を持って処理可能。
-   **Gemini (2048 tokens)**
    -   一般的なRAGのチャンク（500〜1000文字程度）であれば問題ないが、論文全体や長文の議事録を分割せずに処理しようとすると制限にかかる可能性がある。
    -   *対策*: チャンク分割ロジック（`SemanticCoverage`等）で、トークン数が2048を超えないよう厳密に制御する必要がある。

### 3.2 バッチ処理（スピードとスループット）

-   **OpenAI**
    -   1回のリクエストで最大2048個のテキストを送信可能。
    -   数万件のデータをQdrantへ登録する際、APIコール回数を最小限に抑えられ、全体の処理時間を短縮できる。
-   **Gemini**
    -   `batchEmbedContents` メソッドの上限が **100件** と比較的少ない。
    -   OpenAIと同量を処理する場合、リクエスト回数が約20倍となるため、クライアント側（Python）で適切に分割（チャンキング）して送信するループ処理の実装が必須である。

### 3.3 次元数とデータベース容量・精度

-   **Gemini (3072次元)**
    -   **高次元**: デフォルトで3072次元の高密度なベクトルを生成し、テキストの意味的なニュアンスを詳細に捉えることが期待できる。
    -   **MRL (Matryoshka Representation Learning)**: この機能により、精度を維持したまま768次元や1536次元への「切り詰め（truncation）」が可能。ストレージ容量や検索速度の要件に応じて柔軟に次元数を選択できる点が大きな強みである。
-   **OpenAI (1536次元)**
    -   標準的な次元数であり、多くのベクトルDBでデフォルトとしてサポートされている。こちらもパラメータ指定による次元数削減に対応している。

### 3.4 コストパフォーマンス

-   **商用利用 (Vertex AI vs OpenAI API)**
    -   OpenAI `text-embedding-3-small` は非常に安価（1Mトークンあたり約3円）。
    -   Gemini (Vertex AI) は文字数課金であり、トークン換算ではOpenAIの方が有利なケースが多い。
-   **無料枠 (AI Studio)**
    -   Google AI Studio経由でGeminiを利用する場合、レート制限内であれば**無料**で利用可能。開発・検証フェーズや小規模運用においては圧倒的なコストメリットがある。

---

## 4. 結論と推奨

本プロジェクトにおける推奨方針は以下の通りである。

1.  **Gemini (`gemini-embedding-001`) の継続利用**
    -   特に日本語の検索タスク（Retrieval）において高い性能が期待できるため、**3072次元**での運用を推奨する。
    -   これにより、RAGシステムの検索精度（Relevantな情報の取得率）を最大化できる。

2.  **実装上の留意点**
    -   `helper_embedding.py` では、Geminiのバッチサイズ制限（100件）を考慮した実装を維持すること。
    -   入力テキストが2048トークンを超えないよう、前処理段階でのチャンク分割を徹底すること。

3.  **将来的な最適化**
    -   Qdrantのストレージコストや検索速度が課題となった場合、MRL機能を利用して次元数を1536または768に縮小することを検討する（ただし、コレクションの再作成が必要）。
