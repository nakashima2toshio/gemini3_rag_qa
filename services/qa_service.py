#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
qa_service.py - Q/Aç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹
================================
Q/Aãƒšã‚¢ã®ç”Ÿæˆã¨ä¿å­˜ã«é–¢ã™ã‚‹ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯

æ©Ÿèƒ½:
- a02_make_qa_para.pyã®ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
- OpenAI APIã«ã‚ˆã‚‹Q/Aç”Ÿæˆ
- Q/Aãƒšã‚¢ã®ä¿å­˜
"""

import os
import sys
import re
import json
import logging
import subprocess
import threading
import queue
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

import pandas as pd
from helper_llm import create_llm_client

# ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from models import QAPair, QAPairsResponse

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)


def run_advanced_qa_generation(
    dataset: Optional[str],
    input_file: Optional[str],
    use_celery: bool,
    celery_workers: int,
    batch_chunks: int,
    max_docs: int,
    merge_chunks: bool,
    min_tokens: int,
    max_tokens: int,
    coverage_threshold: float,
    model: str,
    analyze_coverage: bool,
    log_callback,
    progress_callback=None,
) -> Dict[str, Any]:
    """
    a02_make_qa_para.pyã‚’ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œ

    æ”¹å–„å†…å®¹ï¼ˆ2024å¹´11æœˆ26æ—¥ï¼‰ï¼š
    - Redisç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã«ã‚ˆã‚‹ç¢ºå®Ÿãªçµæœåé›†
    - ã‚¿ã‚¹ã‚¯çŠ¶æ…‹èª¤èªè­˜ï¼ˆPENDINGï¼‰ã®å›é¿
    - ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†æ™‚ã®Celeryæ¥ç¶šã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    - å…¨1612ã‚¿ã‚¹ã‚¯ã®æ­£å¸¸å®Œäº†ã‚’ä¿è¨¼

    Args:
        dataset: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        input_file: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        use_celery: Celeryä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨ï¼ˆRedisç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹å¯¾å¿œï¼‰
        celery_workers: Celeryãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        batch_chunks: ãƒãƒƒãƒãƒãƒ£ãƒ³ã‚¯æ•°
        max_docs: æœ€å¤§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        merge_chunks: ãƒãƒ£ãƒ³ã‚¯çµ±åˆ
        min_tokens: æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        coverage_threshold: ã‚«ãƒãƒ¬ãƒ¼ã‚¸é–¾å€¤
        model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆgpt-5ã‚·ãƒªãƒ¼ã‚ºã€O-serieså¯¾å¿œï¼‰
        analyze_coverage: ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æã‚’å®Ÿè¡Œ
        log_callback: ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
        progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (current, total) -> None

    Returns:
        å®Ÿè¡Œçµæœã®è¾æ›¸
    """
    # ã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰
    cmd = [sys.executable, "a02_make_qa_para.py"]

    if dataset:
        cmd.extend(["--dataset", dataset])
    elif input_file:
        cmd.extend(["--input-file", input_file])

    if use_celery:
        cmd.append("--use-celery")
        cmd.extend(["--celery-workers", str(celery_workers)])

    cmd.extend(
        [
            "--batch-chunks",
            str(batch_chunks),
            "--max-docs",
            str(max_docs),
            "--min-tokens",
            str(min_tokens),
            "--max-tokens",
            str(max_tokens),
            "--coverage-threshold",
            str(coverage_threshold),
            "--model",
            model,
        ]
    )

    if merge_chunks:
        cmd.append("--merge-chunks")

    if analyze_coverage:
        cmd.append("--analyze-coverage")

    # ç’°å¢ƒå¤‰æ•°ã‚’ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰ã‚³ãƒ”ãƒ¼
    env = os.environ.copy()

    log_callback(f"ğŸš€ é«˜åº¦ãªQ/Aç”Ÿæˆã‚’é–‹å§‹: {' '.join(cmd)}")

    # å‡ºåŠ›ã‚’ã‚­ãƒ¥ãƒ¼ã«æ ¼ç´
    output_queue = queue.Queue()

    def read_output(pipe, q):
        """ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã®å‡ºåŠ›ã‚’èª­ã¿å–ã‚‹"""
        for line in iter(pipe.readline, ""):
            if line:
                q.put(line.strip())
        pipe.close()

    try:
        # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True,
            bufsize=1,
            env=env,
        )

        # å‡ºåŠ›èª­ã¿å–ã‚Šã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        read_thread = threading.Thread(
            target=read_output, args=(process.stdout, output_queue)
        )
        read_thread.daemon = True
        read_thread.start()

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ­ã‚°ã‚’å‡¦ç†
        saved_files = None
        qa_count = 0
        coverage_results = None

        while True:
            # ãƒ—ãƒ­ã‚»ã‚¹ãŒçµ‚äº†ã—ãŸã‹ãƒã‚§ãƒƒã‚¯
            poll = process.poll()

            # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å‡ºåŠ›ã‚’å–å¾—
            try:
                line = output_queue.get(timeout=0.1)
                log_callback(line)

                # é€²æ—æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if progress_callback:
                    # "é€²æ—: å®Œäº†=123/305" ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒ
                    progress_match = re.search(
                        r"é€²æ—.*?å®Œäº†[=:ï¼š\s]*(\d+)\s*/\s*(\d+)", line
                    )
                    if progress_match:
                        current = int(progress_match.group(1))
                        total = int(progress_match.group(2))
                        progress_callback(current, total)

                # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŠ½å‡º
                if "CSVä¿å­˜:" in line:
                    csv_match = line.split("CSVä¿å­˜:")[-1].strip()
                    if saved_files is None:
                        saved_files = {}
                    saved_files["csv"] = f"qa_output/{csv_match}"

                elif "JSONä¿å­˜:" in line:
                    json_match = line.split("JSONä¿å­˜:")[-1].strip()
                    if saved_files:
                        saved_files["json"] = f"qa_output/{json_match}"

                elif "ç”ŸæˆQ/Aãƒšã‚¢æ•°:" in line or "ç”ŸæˆQ/Aãƒšã‚¢:" in line:
                    # Q/Aæ•°ã‚’æŠ½å‡º
                    # "ç”ŸæˆQ/Aãƒšã‚¢æ•°: 118" ã¾ãŸã¯ "ç”ŸæˆQ/Aãƒšã‚¢: 118å€‹" ã®ä¸¡æ–¹ã«å¯¾å¿œ
                    count_match = re.search(r"(\d+)", line)
                    if count_match:
                        qa_count = int(count_match.group(1))

                elif "ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡:" in line:
                    # ã‚«ãƒãƒ¬ãƒ¼ã‚¸çµæœã‚’è§£æ
                    rate_match = re.search(r"([\d.]+)%", line)
                    if rate_match:
                        coverage_results = {
                            "coverage_rate": float(rate_match.group(1)) / 100
                        }

            except queue.Empty:
                pass

            # ãƒ—ãƒ­ã‚»ã‚¹ãŒçµ‚äº†ã—ãŸã‚‰æ®‹ã‚Šã®å‡ºåŠ›ã‚’å‡¦ç†
            if poll is not None:
                # æ®‹ã‚Šã®å‡ºåŠ›ã‚’å…¨ã¦å–å¾—
                while not output_queue.empty():
                    try:
                        line = output_queue.get_nowait()
                        log_callback(line)
                    except queue.Empty:
                        break
                break

        # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã‚³ãƒ¼ãƒ‰ç¢ºèª
        return_code = process.returncode

        if return_code == 0:
            log_callback("âœ… é«˜åº¦ãªQ/Aç”ŸæˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            return {
                "success": True,
                "saved_files": saved_files,
                "qa_count": qa_count,
                "coverage_results": coverage_results,
            }
        else:
            log_callback(f"âš ï¸ é«˜åº¦ãªQ/Aç”ŸæˆãŒçµ‚äº†ã‚³ãƒ¼ãƒ‰ {return_code} ã§çµ‚äº†ã—ã¾ã—ãŸ")
            return {"success": False, "return_code": return_code}

    except Exception as e:
        log_callback(f"âŒ é«˜åº¦ãªQ/Aç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return {"success": False, "error": str(e)}


def generate_qa_pairs(
    text: str,
    dataset_type: str,
    chunk_id: str,
    model: str = "gemini-2.0-flash",
    qa_per_chunk: int = 3,
    log_callback=None,
) -> List[QAPair]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰Q/Aãƒšã‚¢ã‚’ç”Ÿæˆï¼ˆGemini APIä½¿ç”¨ï¼‰

    Args:
        text: å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        chunk_id: ãƒãƒ£ãƒ³ã‚¯ID
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.0-flashï¼‰
        qa_per_chunk: ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®Q/Aæ•°
        log_callback: ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°

    Returns:
        Q/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    # Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
    client = create_llm_client(provider="gemini")

    prompt = f"""ã‚ãªãŸã¯æ•™è‚²ç”¨Q/Aãƒšã‚¢ç”Ÿæˆã®å°‚é–€å®¶ã§ã™ã€‚

ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€{qa_per_chunk}å€‹ã®è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

è¦ä»¶:
1. è³ªå•ã¯å…·ä½“çš„ã§æ˜ç¢ºãªã‚‚ã®ã«ã™ã‚‹
2. å›ç­”ã¯ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«åŸºã¥ã„ãŸæ­£ç¢ºãªã‚‚ã®ã«ã™ã‚‹
3. è³ªå•ã‚¿ã‚¤ãƒ—ã¯ä»¥ä¸‹ã‹ã‚‰é¸æŠ: factual, conceptual, application, analysis
4. ãƒ†ã‚­ã‚¹ãƒˆã®é‡è¦ãªæƒ…å ±ã‚’ç¶²ç¾…ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹

JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

    try:
        # Geminiæ§‹é€ åŒ–å‡ºåŠ›APIã‚’ä½¿ç”¨
        qa_response = client.generate_structured(
            prompt=prompt,
            response_schema=QAPairsResponse,
            model=model
        )

        # Q/Aãƒšã‚¢ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        result_pairs = []
        for qa in qa_response.qa_pairs:
            qa_pair = QAPair(
                question=qa.question,
                answer=qa.answer,
                question_type=qa.question_type,
                source_chunk_id=chunk_id,
                dataset_type=dataset_type,
                auto_generated=True
            )
            result_pairs.append(qa_pair)

        if log_callback:
            log_callback(f"    â””â”€ {len(result_pairs)}å€‹ã®Q/Aãƒšã‚¢ã‚’ç”Ÿæˆ")

        return result_pairs

    except Exception as e:
        logger.error(f"Q/Aç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        if log_callback:
            log_callback(f"    â””â”€ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []


def save_qa_pairs_to_file(
    qa_pairs: List[QAPair], dataset_type: str, log_callback=None
) -> Dict[str, str]:
    """
    Q/Aãƒšã‚¢ã‚’CSVã¨JSONã§ä¿å­˜

    Args:
        qa_pairs: Q/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        log_callback: ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°

    Returns:
        ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¾æ›¸
    """
    qa_output_dir = Path("qa_output")
    qa_output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    saved_files = {}

    # DataFrameã«å¤‰æ›
    qa_data = []
    for qa in qa_pairs:
        qa_data.append(
            {
                "question": qa.question,
                "answer": qa.answer,
                "question_type": qa.question_type,
                "source_chunk_id": qa.source_chunk_id,
                "dataset_type": qa.dataset_type,
                "auto_generated": qa.auto_generated,
            }
        )

    df_qa = pd.DataFrame(qa_data)

    # CSVãƒ•ã‚¡ã‚¤ãƒ«
    csv_filename = f"qa_pairs_{dataset_type}_{timestamp}.csv"
    csv_path = qa_output_dir / csv_filename
    df_qa.to_csv(csv_path, index=False, encoding="utf-8-sig")
    saved_files["csv"] = str(csv_path)

    if log_callback:
        log_callback(f"  ğŸ“„ CSVä¿å­˜: {csv_filename}")

    # JSONãƒ•ã‚¡ã‚¤ãƒ«
    json_filename = f"qa_pairs_{dataset_type}_{timestamp}.json"
    json_path = qa_output_dir / json_filename

    json_data = {
        "dataset_type": dataset_type,
        "created_at": datetime.now().isoformat(),
        "total_pairs": len(qa_pairs),
        "qa_pairs": qa_data,
    }

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    saved_files["json"] = str(json_path)

    if log_callback:
        log_callback(f"  ğŸ“‹ JSONä¿å­˜: {json_filename}")

    return saved_files