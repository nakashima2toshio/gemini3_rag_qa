#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
celery_tasks.py - CeleryéåŒæœŸã‚¿ã‚¹ã‚¯å®šç¾©
=========================================
Q/Aãƒšã‚¢ç”Ÿæˆã®ä¸¦åˆ—å‡¦ç†ã®ãŸã‚ã®Celeryã‚¿ã‚¹ã‚¯å®šç¾©
"""

import os
import json
import logging
from typing import List, Dict, Optional
from celery import Celery
from dotenv import load_dotenv
# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from models import QAPairsResponse
from config import ModelConfig, CeleryConfig

# =====================================================
# Gemini 3 Migration: æŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼
# =====================================================
from helper_llm import create_llm_client, LLMClient

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°ã§è¨­å®šå¯èƒ½ï¼‰
DEFAULT_LLM_PROVIDER = os.getenv("LLM_PROVIDER", "gemini")  # "gemini" or "openai"

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Celeryã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
app = Celery(
    'qa_generation',
    broker=os.getenv('CELERY_BROKER_URL', CeleryConfig.BROKER_URL),
    backend=os.getenv('CELERY_RESULT_BACKEND', CeleryConfig.RESULT_BACKEND)
)

# Celeryè¨­å®š
app.conf.update(
    task_serializer=CeleryConfig.TASK_SERIALIZER,
    accept_content=CeleryConfig.ACCEPT_CONTENT,
    result_serializer=CeleryConfig.RESULT_SERIALIZER,
    timezone=CeleryConfig.TIMEZONE,
    enable_utc=CeleryConfig.ENABLE_UTC,
    # ã‚¿ã‚¹ã‚¯ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
    task_time_limit=CeleryConfig.TASK_TIME_LIMIT,
    task_soft_time_limit=CeleryConfig.TASK_SOFT_TIME_LIMIT,
    # ä¸¦åˆ—åº¦ã®åˆ¶å¾¡
    worker_concurrency=CeleryConfig.WORKER_CONCURRENCY,
    worker_prefetch_multiplier=CeleryConfig.WORKER_PREFETCH_MULTIPLIER,
    # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
    task_acks_late=True,
    task_reject_on_worker_lost=True,
)


# ===========================================
# ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶ç´„ï¼ˆconfig.pyã‹ã‚‰å‚ç…§ï¼‰
# ===========================================
def supports_temperature(model: str) -> bool:
    """ãƒ¢ãƒ‡ãƒ«ãŒtemperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    return ModelConfig.supports_temperature(model)


def determine_qa_count(chunk_data: Dict, config: Dict) -> int:
    """
    ãƒãƒ£ãƒ³ã‚¯ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ã„ã¦Q/Aæ•°ã‚’æ±ºå®š

    Args:
        chunk_data: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        config: è¨­å®šæƒ…å ±

    Returns:
        ç”Ÿæˆã™ã‚‹Q/Aæ•°
    """
    tokens = chunk_data.get('tokens', 100)
    base_qa_count = config.get('qa_per_chunk', 2)

    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«å¿œã˜ã¦èª¿æ•´
    if tokens < 50:
        return max(1, base_qa_count - 1)
    elif tokens > 150:
        return base_qa_count + 1
    else:
        return base_qa_count


def _extract_parsed_response(response, model: str) -> QAPairsResponse:
    """
    responses.parse() API ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰è§£ææ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º

    Args:
        response: OpenAI API ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        model: ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«å

    Returns:
        QAPairsResponse ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    Raises:
        ValueError: è§£æå¯èƒ½ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãªã„å ´åˆ
    """
    parsed_response = None

    # ãƒ‡ãƒãƒƒã‚°: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ§‹é€ ã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ—: {type(response).__name__}")
    response_attrs = [attr for attr in dir(response) if not attr.startswith('_')]
    logger.info(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å±æ€§: {response_attrs}")

    # æ–¹æ³•1: output_parsedå±æ€§ã‚’ç›´æ¥ç¢ºèªï¼ˆGPT-5ã‚·ãƒªãƒ¼ã‚ºå¯¾å¿œï¼‰
    if hasattr(response, 'output_parsed') and response.output_parsed:
        parsed_response = response.output_parsed
        logger.info("GPT-5å½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆoutput_parsedï¼‰ã‚’ä½¿ç”¨")
        return parsed_response

    # æ–¹æ³•2: outputé…åˆ—ã‹ã‚‰æ¢ç´¢ï¼ˆGPT-4oå¯¾å¿œï¼‰
    if hasattr(response, 'output'):
        logger.info(f"outputé…åˆ—ã®é•·ã•: {len(response.output)}")
        for idx, output in enumerate(response.output):
            output_type = getattr(output, 'type', 'N/A')
            logger.info(f"output[{idx}] ã‚¿ã‚¤ãƒ—: {type(output).__name__}, typeå±æ€§: {output_type}")

            # ReasoningItemã¯ã‚¹ã‚­ãƒƒãƒ—
            if hasattr(output, 'type'):
                if output.type == "reasoning":
                    logger.info("  -> reasoning ã‚¿ã‚¤ãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                    continue
                elif output.type == "message" and hasattr(output, 'content') and output.content:
                    logger.info(f"  -> message ã‚¿ã‚¤ãƒ—ã€contenté•·ã•: {len(output.content)}")
                    for item_idx, item in enumerate(output.content):
                        item_type = getattr(item, 'type', 'N/A')
                        logger.info(f"    content[{item_idx}] ã‚¿ã‚¤ãƒ—: {type(item).__name__}, typeå±æ€§: {item_type}")

                        # parsedå±æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                        if hasattr(item, 'type') and item.type == "output_text" and hasattr(item, 'parsed') and item.parsed:
                            parsed_response = item.parsed
                            logger.info("GPT-4oå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆoutputé…åˆ— -> parsedï¼‰ã‚’ä½¿ç”¨")
                            return parsed_response

                        # textå±æ€§ãŒã‚ã‚‹å ´åˆã‚‚ãƒ­ã‚°å‡ºåŠ›
                        if hasattr(item, 'text') and item.text:
                            text_preview = item.text[:100] if len(item.text) > 100 else item.text
                            logger.info(f"    textå±æ€§ã‚ã‚Šï¼ˆæœ€åˆã®100æ–‡å­—ï¼‰: {text_preview}...")

            if parsed_response:
                break

    # æ–¹æ³•3: ç›´æ¥textå±æ€§ã‹ã‚‰JSONè§£æã‚’è©¦ã¿ã‚‹
    if not parsed_response and hasattr(response, 'output'):
        logger.info("æ–¹æ³•3: textå±æ€§ã‹ã‚‰JSONç›´æ¥è§£æã‚’è©¦è¡Œ")
        for output in response.output:
            if hasattr(output, 'type') and output.type == "message" and hasattr(output, 'content'):
                for item in output.content:
                    if hasattr(item, 'text') and item.text:
                        try:
                            json_data = json.loads(item.text)
                            if 'qa_pairs' in json_data:
                                # JSONã‹ã‚‰QAPairsResponseã‚’æ§‹ç¯‰
                                parsed_response = QAPairsResponse(**json_data)
                                logger.info("JSONãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥è§£ææˆåŠŸ")
                                return parsed_response
                        except json.JSONDecodeError as e:
                            logger.debug(f"JSONè§£æå¤±æ•—ï¼ˆJSONDecodeErrorï¼‰: {str(e)[:100]}")
                        except Exception as e:
                            logger.debug(f"JSONè§£æå¤±æ•—ï¼ˆãã®ä»–ï¼‰: {str(e)[:100]}")
            if parsed_response:
                break

    # æ–¹æ³•4: output_textå±æ€§ã‚’ç›´æ¥ç¢ºèª
    if not parsed_response and hasattr(response, 'output_text') and response.output_text:
        logger.info("æ–¹æ³•4: output_textå±æ€§ã‹ã‚‰JSONè§£æã‚’è©¦è¡Œ")
        try:
            json_data = json.loads(response.output_text)
            if 'qa_pairs' in json_data:
                parsed_response = QAPairsResponse(**json_data)
                logger.info("output_textã‹ã‚‰ç›´æ¥è§£ææˆåŠŸ")
                return parsed_response
        except Exception as e:
            logger.debug(f"output_textè§£æå¤±æ•—: {str(e)[:100]}")

    if not parsed_response:
        logger.error("OpenAI APIãŒè§£æå¯èƒ½ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã—ã¾ã›ã‚“ã§ã—ãŸ")
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        try:
            response_str = str(response)[:1000]
            logger.error(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…¨ä½“ï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰: {response_str}")
        except Exception:
            pass
        raise ValueError("No parsable response from OpenAI API")

    return parsed_response


@app.task(bind=True, max_retries=3)
def generate_qa_for_chunk_async(self, chunk_data: Dict, config: Dict, model: str = "gemini-2.0-flash") -> Dict:
    """
    å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰Q/Aãƒšã‚¢ã‚’éåŒæœŸç”Ÿæˆï¼ˆCeleryã‚¿ã‚¹ã‚¯ï¼‰

    â€» å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°ã€‚å†…éƒ¨ã§generate_qa_unified_asyncã‚’å‘¼ã³å‡ºã™ã€‚

    Args:
        chunk_data: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.0-flashï¼‰

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã¨é–¢é€£æƒ…å ±ã‚’å«ã‚€è¾æ›¸
    """
    logger.info(f"[å¾Œæ–¹äº’æ›ãƒ©ãƒƒãƒ‘ãƒ¼] generate_qa_for_chunk_async -> generate_qa_unified_async")
    # çµ±åˆã‚¿ã‚¹ã‚¯ã«å§”è­²ï¼ˆGeminiã‚’ä½¿ç”¨ï¼‰
    return generate_qa_unified_async(chunk_data, config, model=None, provider="gemini")


@app.task(bind=True, max_retries=3)
def generate_qa_for_batch_async(self, chunks: List[Dict], config: Dict, model: str = "gemini-2.0-flash") -> Dict:
    """
    è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰Q/Aãƒšã‚¢ã‚’éåŒæœŸãƒãƒƒãƒç”Ÿæˆï¼ˆCeleryã‚¿ã‚¹ã‚¯ï¼‰

    â€» å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°ã€‚å„ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦generate_qa_unified_asyncã‚’å‘¼ã³å‡ºã™ã€‚

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆ1-5å€‹ï¼‰
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.0-flashï¼‰

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã¨é–¢é€£æƒ…å ±ã‚’å«ã‚€è¾æ›¸
    """
    logger.info(f"[å¾Œæ–¹äº’æ›ãƒ©ãƒƒãƒ‘ãƒ¼] generate_qa_for_batch_async -> generate_qa_unified_async (è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯)")

    chunk_ids = [c.get('id', 'unknown') for c in chunks]
    all_qa_pairs = []

    try:
        # å„ãƒãƒ£ãƒ³ã‚¯ã«å¯¾ã—ã¦çµ±åˆã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        for chunk in chunks:
            result = generate_qa_unified_async(chunk, config, model=None, provider="gemini")
            if result.get('success'):
                all_qa_pairs.extend(result.get('qa_pairs', []))

        logger.info(f"[å¾Œæ–¹äº’æ›ãƒãƒƒãƒ] å®Œäº†: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ - {len(all_qa_pairs)}å€‹ã®Q/Aç”Ÿæˆ")

        return {
            "success": True,
            "chunk_ids": chunk_ids,
            "qa_pairs": all_qa_pairs,
            "error": None
        }

    except Exception as e:
        logger.error(f"[å¾Œæ–¹äº’æ›ãƒãƒƒãƒ] ã‚¨ãƒ©ãƒ¼: {str(e)}")

        # ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
        if self.request.retries < self.max_retries:
            raise self.retry(exc=e, countdown=5 * (self.request.retries + 1))

        return {
            "success": False,
            "chunk_ids": chunk_ids,
            "qa_pairs": all_qa_pairs,
            "error": str(e)
        }


# =====================================================
# Gemini 3 Migration: çµ±åˆQ/Aç”Ÿæˆã‚¿ã‚¹ã‚¯
# =====================================================

@app.task(bind=True, max_retries=3)
def generate_qa_unified_async(
    self,
    chunk_data: Dict,
    config: Dict,
    model: str = None,
    provider: str = None
) -> Dict:
    """
    å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰Q/Aãƒšã‚¢ã‚’éåŒæœŸç”Ÿæˆï¼ˆçµ±åˆç‰ˆ: Gemini/OpenAIå¯¾å¿œï¼‰

    Gemini 3 Migration: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ã¦Geminiã¾ãŸã¯OpenAIã‚’ä½¿ç”¨

    Args:
        chunk_data: ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆNoneã®å ´åˆã¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        provider: "gemini" or "openai"ï¼ˆNoneã®å ´åˆã¯DEFAULT_LLM_PROVIDERï¼‰

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã¨é–¢é€£æƒ…å ±ã‚’å«ã‚€è¾æ›¸
    """
    try:
        provider = provider or DEFAULT_LLM_PROVIDER

        # Geminiãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä½¿ç”¨æ™‚ã«OpenAIãƒ¢ãƒ‡ãƒ«åãŒæ¸¡ã•ã‚ŒãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        if provider == "gemini" and model and ("gpt" in model.lower() or "o1" in model.lower() or "o3" in model.lower() or "o4" in model.lower()):
            logger.warning(f"[çµ±åˆã‚¿ã‚¹ã‚¯] OpenAIãƒ¢ãƒ‡ãƒ« '{model}' ã¯Geminiãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            model = None  # Noneã«ã™ã‚‹ã“ã¨ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®gemini-2.0-flashã‚’ä½¿ç”¨

        logger.info(f"[çµ±åˆã‚¿ã‚¹ã‚¯] ãƒãƒ£ãƒ³ã‚¯ {chunk_data.get('id', 'unknown')}, ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}, ãƒ¢ãƒ‡ãƒ«: {model or 'default'}")

        # Q/Aæ•°ã®æ±ºå®š
        num_pairs = determine_qa_count(chunk_data, config)
        lang = config["lang"]

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šï¼ˆè¨€èªåˆ¥ï¼‰
        if lang == "ja":
            system_instruction = """ã‚ãªãŸã¯æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã®å°‚é–€å®¶ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€å­¦ç¿’åŠ¹æœã®é«˜ã„Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
è³ªå•ã¯æ˜ç¢ºã§å…·ä½“çš„ã«ã€å›ç­”ã¯ç°¡æ½”ã§æ­£ç¢ºã«ï¼ˆ1-2æ–‡ç¨‹åº¦ï¼‰ã€‚"""

            prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰{num_pairs}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è³ªå•ã‚¿ã‚¤ãƒ—: factï¼ˆäº‹å®Ÿç¢ºèªï¼‰, reasonï¼ˆç†ç”±èª¬æ˜ï¼‰, comparisonï¼ˆæ¯”è¼ƒï¼‰, applicationï¼ˆå¿œç”¨ï¼‰

ãƒ†ã‚­ã‚¹ãƒˆ:
{chunk_data['text']}

JSONå½¢å¼ã§å‡ºåŠ›:
{{"qa_pairs": [{{"question": "è³ªå•æ–‡", "answer": "å›ç­”æ–‡", "question_type": "fact/reason/comparison/application"}}]}}"""
        else:
            system_instruction = """You are an expert in educational content creation.
Generate high-quality Q&A pairs from the given English text.
Questions should be clear and specific, answers concise and accurate (1-2 sentences)."""

            prompt = f"""Generate {num_pairs} Q&A pairs from the following text.

Question types: fact, reason, comparison, application

Text:
{chunk_data['text']}

Output in JSON format:
{{"qa_pairs": [{{"question": "question text", "answer": "answer text", "question_type": "fact/reason/comparison/application"}}]}}"""

        # çµ±åˆLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
        llm_client = create_llm_client(provider=provider)

        # æ§‹é€ åŒ–å‡ºåŠ›ã‚’è©¦è¡Œ
        try:
            result = llm_client.generate_structured(
                prompt=f"{system_instruction}\n\n{prompt}",
                response_schema=QAPairsResponse,
                model=model
            )

            qa_pairs = []
            for qa_data in result.qa_pairs:
                qa = {
                    "question": qa_data.question,
                    "answer": qa_data.answer,
                    "question_type": qa_data.question_type,
                    "source_chunk_id": chunk_data.get('id', ''),
                    "doc_id": chunk_data.get('doc_id', ''),
                    "dataset_type": chunk_data.get('dataset_type', ''),
                    "chunk_idx": chunk_data.get('chunk_idx', 0),
                    "provider": provider  # ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¨˜éŒ²
                }
                qa_pairs.append(qa)

        except Exception as e:
            logger.warning(f"æ§‹é€ åŒ–å‡ºåŠ›å¤±æ•—ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {str(e)[:100]}")

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã—ã¦JSONè§£æ
            response_text = llm_client.generate_content(
                prompt=f"{system_instruction}\n\n{prompt}",
                model=model
            )

            # JSONã‚’æŠ½å‡ºã—ã¦è§£æ
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                parsed_data = json.loads(json_match.group())
                qa_pairs = []
                for qa_data in parsed_data.get('qa_pairs', []):
                    qa = {
                        "question": qa_data.get('question', ''),
                        "answer": qa_data.get('answer', ''),
                        "question_type": qa_data.get('question_type', 'fact'),
                        "source_chunk_id": chunk_data.get('id', ''),
                        "doc_id": chunk_data.get('doc_id', ''),
                        "dataset_type": chunk_data.get('dataset_type', ''),
                        "chunk_idx": chunk_data.get('chunk_idx', 0),
                        "provider": provider
                    }
                    qa_pairs.append(qa)
            else:
                raise ValueError("JSON not found in response")

        logger.info(f"[çµ±åˆã‚¿ã‚¹ã‚¯] å®Œäº†: {len(qa_pairs)}å€‹ã®Q/Aç”Ÿæˆ")

        return {
            "success": True,
            "chunk_id": chunk_data.get('id'),
            "qa_pairs": qa_pairs,
            "provider": provider,
            "error": None
        }

    except Exception as e:
        logger.error(f"[çµ±åˆã‚¿ã‚¹ã‚¯] ã‚¨ãƒ©ãƒ¼: {str(e)}")

        # ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
        if self.request.retries < self.max_retries:
            raise self.retry(exc=e, countdown=5 * (self.request.retries + 1))

        return {
            "success": False,
            "chunk_id": chunk_data.get('id'),
            "qa_pairs": [],
            "provider": provider or DEFAULT_LLM_PROVIDER,
            "error": str(e)
        }


def submit_unified_qa_generation(
    chunks: List[Dict],
    config: Dict,
    model: str = None,
    provider: str = None
) -> List:
    """
    çµ±åˆQ/Aç”Ÿæˆã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥ï¼ˆGemini/OpenAIå¯¾å¿œï¼‰

    Gemini 3 Migration: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠå¯èƒ½

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆNoneã®å ´åˆã¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        provider: "gemini" or "openai"ï¼ˆNoneã®å ´åˆã¯DEFAULT_LLM_PROVIDERï¼‰

    Returns:
        Celeryã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ

    Example:
        # Geminiä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        tasks = submit_unified_qa_generation(chunks, config)

        # OpenAIä½¿ç”¨
        tasks = submit_unified_qa_generation(chunks, config, provider="openai")
    """
    provider = provider or DEFAULT_LLM_PROVIDER
    tasks = []

    for chunk in chunks:
        task = generate_qa_unified_async.apply_async(
            args=[chunk, config, model, provider],
            queue='qa_generation'
        )
        tasks.append(task)

    logger.info(f"[çµ±åˆQ/Aç”Ÿæˆ] æŠ•å…¥ã‚¿ã‚¹ã‚¯æ•°: {len(tasks)}, ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")
    return tasks


def submit_parallel_qa_generation(chunks: List[Dict], config: Dict, model: str = "gemini-2.0-flash",
                                 batch_size: int = 3) -> List:
    """
    ä¸¦åˆ—Q/Aç”Ÿæˆã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥

    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆ1-5ï¼‰

    Returns:
        Celeryã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    tasks = []

    # ãƒãƒƒãƒå‡¦ç†ã®å ´åˆ
    if batch_size > 1:
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            task = generate_qa_for_batch_async.apply_async(
                args=[batch, config, model],
                queue='qa_generation'  # ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒç›£è¦–ã—ã¦ã„ã‚‹ã‚­ãƒ¥ãƒ¼ã‚’æŒ‡å®š
            )
            tasks.append(task)
            logger.debug(f"ã‚¿ã‚¹ã‚¯æŠ•å…¥: {task.id} - {len(batch)}ãƒãƒ£ãƒ³ã‚¯")
    else:
        # å€‹åˆ¥å‡¦ç†ã®å ´åˆ
        for chunk in chunks:
            task = generate_qa_for_chunk_async.apply_async(
                args=[chunk, config, model],
                queue='qa_generation'  # ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒç›£è¦–ã—ã¦ã„ã‚‹ã‚­ãƒ¥ãƒ¼ã‚’æŒ‡å®š
            )
            tasks.append(task)

    logger.info(f"æŠ•å…¥ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯æ•°: {len(tasks)}")
    return tasks


def collect_results(tasks: List, timeout: int = 300) -> List[Dict]:
    """
    ä¸¦åˆ—å‡¦ç†ã®çµæœã‚’åé›†ï¼ˆæ”¹è‰¯ç‰ˆï¼šRedisç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ç¢ºå®Ÿãªçµæœå–å¾—ï¼‰

    Args:
        tasks: Celeryã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰

    Returns:
        Q/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    import time
    import redis
    import json
    from celery.result import AsyncResult

    all_qa_pairs = []
    failed_chunks = []
    total_tasks = len(tasks)
    completed_tasks = set()  # å®Œäº†æ¸ˆã¿ã‚¿ã‚¹ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    failed_tasks = set()     # å¤±æ•—ã‚¿ã‚¹ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

    logger.info(f"çµæœåé›†é–‹å§‹: {total_tasks}å€‹ã®ã‚¿ã‚¹ã‚¯")

    # Redisæ¥ç¶šã‚’ç¢ºç«‹
    redis_client = redis.Redis(
        host=os.getenv('REDIS_HOST', 'localhost'),
        port=int(os.getenv('REDIS_PORT', 6379)),
        db=int(os.getenv('REDIS_DB', 0)),
        decode_responses=True
    )

    # ã‚¿ã‚¹ã‚¯IDã‚’ãƒ­ã‚°å‡ºåŠ›
    for i, task in enumerate(tasks):
        logger.debug(f"ã‚¿ã‚¹ã‚¯ {i+1}: ID={task.id}")

    start_time = time.time()
    last_log_time = start_time
    stall_check_time = start_time
    last_completed_count = 0
    stall_counter = 0

    # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ«ãƒ¼ãƒ—ã§çµæœã‚’åé›†
    while len(completed_tasks) + len(failed_tasks) < total_tasks:
        current_time = time.time()
        elapsed = current_time - start_time

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
        if elapsed > timeout:
            logger.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {elapsed:.1f}ç§’çµŒé")
            logger.info(f"åé›†æ¸ˆã¿: {len(completed_tasks)}å€‹, æœªåé›†: {total_tasks - len(completed_tasks) - len(failed_tasks)}å€‹")
            break

        # 5ç§’ã”ã¨ã«é€²æ—è¡¨ç¤º
        if current_time - last_log_time >= 5:
            logger.info(f"é€²æ—: å®Œäº†={len(completed_tasks)}/{total_tasks}, "
                       f"å¤±æ•—={len(failed_tasks)}, "
                       f"å‡¦ç†ä¸­={total_tasks - len(completed_tasks) - len(failed_tasks)}, "
                       f"çµŒéæ™‚é–“={elapsed:.1f}ç§’")
            last_log_time = current_time

        # 30ç§’ã”ã¨ã«åœæ»ãƒã‚§ãƒƒã‚¯
        if current_time - stall_check_time >= 30:
            current_completed = len(completed_tasks)
            if current_completed == last_completed_count:
                stall_counter += 1
                logger.warning(f"âš ï¸ å‡¦ç†ãŒåœæ»ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆ{stall_counter * 30}ç§’é–“é€²æ—ãªã—ï¼‰")

                # 3åˆ†é–“é€²æ—ãŒãªã„å ´åˆã€çŠ¶æ…‹ã‚’å¼·åˆ¶ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥
                if stall_counter >= 6:
                    logger.warning("ğŸ“Š ã‚¿ã‚¹ã‚¯çŠ¶æ…‹ã‚’å¼·åˆ¶çš„ã«å†å–å¾—...")
                    for i, task in enumerate(tasks):
                        if i not in completed_tasks and i not in failed_tasks:
                            try:
                                # AsyncResultã§å†å–å¾—
                                refreshed_task = AsyncResult(task.id)
                                if refreshed_task.state in ['SUCCESS', 'FAILURE']:
                                    logger.info(f"ã‚¿ã‚¹ã‚¯ {i+1} ã®çŠ¶æ…‹: {refreshed_task.state}")
                            except Exception as e:
                                logger.debug(f"ã‚¿ã‚¹ã‚¯ {i+1} ã®çŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    stall_counter = 0
            else:
                stall_counter = 0
                last_completed_count = current_completed
            stall_check_time = current_time

        # å„ã‚¿ã‚¹ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
        pending_indices = []
        for i, task in enumerate(tasks):
            # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ã‚¿ã‚¹ã‚¯ã¯ã‚¹ã‚­ãƒƒãƒ—
            if i in completed_tasks or i in failed_tasks:
                continue

            try:
                # ã¾ãšRedisã‹ã‚‰ç›´æ¥çŠ¶æ…‹ã‚’ç¢ºèªï¼ˆæœ€ã‚‚ç¢ºå®Ÿãªæ–¹æ³•ï¼‰
                redis_key = f"celery-task-meta-{task.id}"
                redis_data = redis_client.get(redis_key)

                result = None
                redis_state = None

                if redis_data:
                    try:
                        redis_result = json.loads(redis_data)
                        redis_state = redis_result.get('status')

                        # Redisã§å®Œäº†ã—ã¦ã„ã‚‹å ´åˆã¯çµæœã‚’ç›´æ¥å–å¾—
                        if redis_state == 'SUCCESS':
                            result = redis_result.get('result')
                            if result and isinstance(result, dict):
                                if result.get('success'):
                                    # Redisç›´æ¥å–å¾—æˆåŠŸ
                                    logger.debug(f"ã‚¿ã‚¹ã‚¯ {i+1} Redisç›´æ¥å–å¾—æˆåŠŸ")
                                else:
                                    # ã‚¿ã‚¹ã‚¯ã¯å®Œäº†ã—ãŸãŒã€çµæœãŒsuccess=Falseï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
                                    logger.debug(f"ã‚¿ã‚¹ã‚¯ {i+1} Redisç›´æ¥å–å¾—ï¼ˆå¤±æ•—çµæœï¼‰")
                        elif redis_state == 'FAILURE':
                            # Celeryã‚¿ã‚¹ã‚¯è‡ªä½“ãŒå¤±æ•—
                            failed_tasks.add(i)
                            logger.error(f"âœ— ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} å¤±æ•—ï¼ˆRedis FAILUREçŠ¶æ…‹ï¼‰")
                            continue
                    except json.JSONDecodeError:
                        logger.warning(f"ã‚¿ã‚¹ã‚¯ {i+1} Redis JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼")

                # Redisã‹ã‚‰çµæœã‚’å–å¾—ã§ããŸå ´åˆã¯å‡¦ç†
                if result and isinstance(result, dict):
                    if result.get('success'):
                        # æˆåŠŸã‚¿ã‚¹ã‚¯
                        qa_pairs = result.get('qa_pairs', [])
                        all_qa_pairs.extend(qa_pairs)
                        completed_tasks.add(i)

                        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’ãƒ­ã‚°
                        if 'chunk_id' in result:
                            logger.info(f"âœ“ ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} å®Œäº†ï¼ˆRedisç›´æ¥ï¼‰: ãƒãƒ£ãƒ³ã‚¯ {result['chunk_id']} - {len(qa_pairs)}å€‹ã®Q/A")
                        elif 'chunk_ids' in result:
                            logger.info(f"âœ“ ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} å®Œäº†ï¼ˆRedisç›´æ¥ï¼‰: ãƒãƒƒãƒ {len(result['chunk_ids'])}ãƒãƒ£ãƒ³ã‚¯ - {len(qa_pairs)}å€‹ã®Q/A")
                    else:
                        # å¤±æ•—ã‚¿ã‚¹ã‚¯ï¼ˆsuccess=Falseï¼‰- Redisã‹ã‚‰ç›´æ¥æ¤œå‡º
                        failed_tasks.add(i)
                        error_msg = result.get('error', 'Unknown error')
                        logger.error(f"âœ— ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} å¤±æ•—ï¼ˆRedisç›´æ¥ï¼‰: {error_msg[:200]}")

                        if 'chunk_id' in result:
                            failed_chunks.append(result['chunk_id'])
                        elif 'chunk_ids' in result:
                            failed_chunks.extend(result['chunk_ids'])
                else:
                    # Redisã‹ã‚‰çµæœã‚’å–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯å¾“æ¥ã®æ–¹æ³•ã‚’è©¦ã™
                    # (redis_dataãŒãªã„ã€redis_stateãŒPENDING/STARTEDã€resultãŒNoneç­‰)
                    task_state = task.state

                    # ãƒ‡ãƒãƒƒã‚°: æœ€å¾Œã®ã‚¿ã‚¹ã‚¯ã®çŠ¶æ…‹ã‚’è©³ç´°ãƒ­ã‚°
                    if i == total_tasks - 1:
                        logger.debug(f"ğŸ” æœ€å¾Œã®ã‚¿ã‚¹ã‚¯[{i}] RedisçŠ¶æ…‹={redis_state}, CeleryçŠ¶æ…‹={task_state}, id={task.id[:8]}...")

                    # CeleryçµŒç”±ã§çµæœã‚’å–å¾—
                    if task_state == 'SUCCESS' or task.ready():
                        # çµæœã‚’å–å¾—ï¼ˆçŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰
                        try:
                            result = task.get(timeout=1, propagate=False)
                        except Exception:
                            pass

                        if result and isinstance(result, dict) and result.get('success'):
                            qa_pairs = result.get('qa_pairs', [])
                            all_qa_pairs.extend(qa_pairs)
                            completed_tasks.add(i)

                            # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’ãƒ­ã‚°
                            if 'chunk_id' in result:
                                logger.info(f"âœ“ ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} å®Œäº†: ãƒãƒ£ãƒ³ã‚¯ {result['chunk_id']} - {len(qa_pairs)}å€‹ã®Q/A")
                            elif 'chunk_ids' in result:
                                logger.info(f"âœ“ ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} å®Œäº†: ãƒãƒƒãƒ {len(result['chunk_ids'])}ãƒãƒ£ãƒ³ã‚¯ - {len(qa_pairs)}å€‹ã®Q/A")
                        elif result is not None:
                            failed_tasks.add(i)
                            error_msg = result.get('error', 'Unknown error') if result and isinstance(result, dict) else str(result)
                            logger.error(f"âœ— ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} å¤±æ•—: {error_msg[:200]}")

                            if result and isinstance(result, dict):
                                if 'chunk_id' in result:
                                    failed_chunks.append(result['chunk_id'])
                                elif 'chunk_ids' in result:
                                    failed_chunks.extend(result['chunk_ids'])

                    elif task_state == 'FAILURE':
                        failed_tasks.add(i)
                        logger.error(f"âœ— ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} å¤±æ•—ï¼ˆçŠ¶æ…‹: FAILUREï¼‰")

                    elif task_state == 'PENDING':
                        pending_indices.append(i)

            except TimeoutError:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯æ­£å¸¸ï¼ˆã¾ã å‡¦ç†ä¸­ï¼‰
                pass
            except Exception as e:
                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯1å›ã ã‘ãƒ­ã‚°
                if i not in failed_tasks:
                    logger.debug(f"ã‚¿ã‚¹ã‚¯ {i+1} ãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†ç¶™ç¶šï¼‰: {str(e)[:100]}")

        # ãƒ‡ãƒãƒƒã‚°: ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ã®ã‚¿ã‚¹ã‚¯ãŒå¤šã„å ´åˆè­¦å‘Š
        if len(pending_indices) > 10 and current_time - start_time > 60:
            logger.debug(f"ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ã®ã‚¿ã‚¹ã‚¯æ•°: {len(pending_indices)}")

        # çŸ­ã„å¾…æ©Ÿï¼ˆå‡¦ç†æ¸ˆã¿ã‚¿ã‚¹ã‚¯ãŒå¤šã„å ´åˆã¯å¾…æ©Ÿæ™‚é–“ã‚’é•·ãã™ã‚‹ï¼‰
        completion_ratio = len(completed_tasks) / total_tasks if total_tasks > 0 else 0
        if completion_ratio > 0.9:
            time.sleep(1.0)  # 90%ä»¥ä¸Šå®Œäº†ã—ãŸã‚‰å¾…æ©Ÿæ™‚é–“ã‚’é•·ã
        else:
            time.sleep(0.5)

    # æœ€çµ‚çš„ãªæœªå®Œäº†ã‚¿ã‚¹ã‚¯ã®å‡¦ç†ï¼ˆã‚ˆã‚Šç©æ¥µçš„ã«å–å¾—ï¼‰
    logger.info(f"ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã®æœ€çµ‚ç¢ºèª: æœªå‡¦ç†ã‚¿ã‚¹ã‚¯ {total_tasks - len(completed_tasks) - len(failed_tasks)}å€‹")
    for i, task in enumerate(tasks):
        if i not in completed_tasks and i not in failed_tasks:
            try:
                # AsyncResultã§å¼·åˆ¶çš„ã«æœ€æ–°çŠ¶æ…‹ã‚’å–å¾—
                from celery.result import AsyncResult
                refreshed = AsyncResult(task.id)
                final_state = refreshed.state
                logger.info(f"ã‚¿ã‚¹ã‚¯ {i+1} æœ€çµ‚çŠ¶æ…‹: {final_state}")

                # çŠ¶æ…‹ã«é–¢ã‚ã‚‰ãšçµæœå–å¾—ã‚’è©¦ã¿ã‚‹
                if final_state in ['SUCCESS', 'FAILURE'] or refreshed.ready():
                    result = refreshed.get(timeout=3, propagate=False)
                    if result and isinstance(result, dict) and result.get('success'):
                        qa_pairs = result.get('qa_pairs', [])
                        all_qa_pairs.extend(qa_pairs)
                        completed_tasks.add(i)
                        logger.info(f"âœ“ ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} æœ€çµ‚åé›†ã§å®Œäº†: {len(qa_pairs)}å€‹ã®Q/A")
                    else:
                        failed_tasks.add(i)
                        logger.warning(f"ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} æœ€çµ‚åé›†ã§å¤±æ•—ã¨ã—ã¦å‡¦ç†")
                else:
                    logger.warning(f"ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} æœ€çµ‚çŠ¶æ…‹: {final_state} - æœªå®Œäº†")
            except Exception as e:
                logger.warning(f"ã‚¿ã‚¹ã‚¯ {i+1}/{total_tasks} æœ€çµ‚åé›†ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")

    # çµæœã‚µãƒãƒªãƒ¼
    elapsed_total = time.time() - start_time
    logger.info(f"""
    =====================================
    çµæœåé›†å®Œäº†:
    - æˆåŠŸ: {len(completed_tasks)}/{total_tasks}ã‚¿ã‚¹ã‚¯
    - å¤±æ•—: {len(failed_tasks)}ã‚¿ã‚¹ã‚¯
    - æœªå®Œäº†: {total_tasks - len(completed_tasks) - len(failed_tasks)}ã‚¿ã‚¹ã‚¯
    - ç”ŸæˆQ/Aãƒšã‚¢: {len(all_qa_pairs)}å€‹
    - æ‰€è¦æ™‚é–“: {elapsed_total:.1f}ç§’
    =====================================
    """)

    if failed_chunks:
        logger.warning(f"å¤±æ•—ã—ãŸãƒãƒ£ãƒ³ã‚¯ï¼ˆæœ€åˆã®5å€‹ï¼‰: {failed_chunks[:5]}")

    return all_qa_pairs


if __name__ == "__main__":
    # Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’èµ·å‹•ã™ã‚‹å ´åˆ
    # celery -A celery_tasks worker --loglevel=info --concurrency=4
    pass